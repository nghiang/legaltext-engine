{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717a29e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_7a53abb7a2'}, id='run--6c324ef3-d158-42f0-b5ca-79fedabeaf39-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "AZURE_ENDPOINT = \"\"\n",
    "AZURE_API_KEY = \"\"\n",
    "AZURE_API_VERSION = \"\"\n",
    "AZURE_DEPLOYMENT_NAME =\"\"\n",
    "AZURE_MODEL_NAME = \"\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_version=AZURE_API_VERSION,\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    azure_deployment=AZURE_DEPLOYMENT_NAME,\n",
    "    model=AZURE_MODEL_NAME,\n",
    "    api_key=AZURE_API_KEY,\n",
    "    streaming=True,\n",
    "    temperature=0.0,\n",
    ")\n",
    "llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text successfully saved to ./output/Sb_2000_128_2024-01-01_IZ.txt\n",
      "Text successfully saved to ./output/Sb_2006_183_2023-12-31_IZ.txt\n",
      "Text successfully saved to ./output/Sb_2016_250_2024-01-01_IZ.txt\n",
      "Text successfully saved to ./output/Sb_2009_40_2024-04-01_IZ.txt\n",
      "Text successfully saved to ./output/Sb_2012_90_2024-07-19_IZ.txt\n",
      "Text successfully saved to ./output/Sb_2008_125_2024-07-19_IZ.txt\n",
      "Text successfully saved to ./output/Sb_2021_283_2024-07-01_IZ.txt\n",
      "Text successfully saved to ./output/Sb_1991_455_2024-01-01_IZ.txt\n",
      "Text successfully saved to ./output/Sb_2000_361_2024-07-01_IZ.txt\n",
      "Text successfully saved to ./output/Sb_2012_89_2024-04-01_IZ.txt\n",
      "Text successfully saved to ./output/Sb_2006_262_2024-07-01_IZ.txt\n",
      "Text successfully saved to ./output/Sb_1992_586_2024-07-19_IZ.txt\n",
      "Text successfully saved to ./output/Sb_2012_89_2024-04-01_IZ (1).txt\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import os\n",
    "\n",
    "DATA_FOLDER = \"./data\"\n",
    "OUTPUT_FOLDER = \"./output/text_data\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the extracted text by removing noise and normalizing whitespace.\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = [\n",
    "        line.strip()\n",
    "        for line in lines\n",
    "        if line.strip() and not re.match(r\"^strana \\d+$\", line.strip())\n",
    "    ]\n",
    "    cleaned_text = \"\\n\".join(cleaned_lines)\n",
    "    cleaned_text = re.sub(r\"\\n{2,}\", \"\\n\\n\", cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "def pdf_to_text(file_path, output_path):\n",
    "    \"\"\"Extract text from a PDF file, clean it, and save to a text file.\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text + \"\\n\"\n",
    "\n",
    "        cleaned_text = clean_text(text)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned_text)\n",
    "\n",
    "        print(f\"Text successfully saved to {output_path}\")\n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "pdf_files = [\n",
    "    os.path.join(DATA_FOLDER, fname)\n",
    "    for fname in os.listdir(DATA_FOLDER)\n",
    "    if fname.endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    output_path = os.path.join(OUTPUT_FOLDER, os.path.basename(pdf_file).replace(\".pdf\", \".txt\"))\n",
    "    pdf_to_text(pdf_file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b17dc728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 3050/3050 [26:31<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON saved to './output/json_data/Sb_1991_455_2024-01-01_IZ.json'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated\n",
    "import tqdm\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "part_pattern = r\"^ČÁST\\s+.*\"\n",
    "chapter_pattern = r\"^HLAVA\\s+.*\"\n",
    "section_pattern = r\"^DÍL\\s+.*\"\n",
    "subsection_pattern = r\"^ODDÍL\\s+.*\"\n",
    "paragraph_pattern = r\"^§\\s+\\d+.*\"\n",
    "subsubsection_pattern = r\"^\\((\\d+)\\)\\s*(.*)\"\n",
    "appendix_pattern = r\"^Příloha\\s+č\\.\\s*\\d+\\s*k zákonu\\s*č\\.\\s*\\d+/\\d+\\s*Sb\\.\"\n",
    "from typing import Literal\n",
    "CONTENT_TYPES = {\n",
    "    \"law\": \"law\",\n",
    "    \"part\": \"part\",\n",
    "    \"chapter\": \"chapter\",\n",
    "    \"section\": \"section\",\n",
    "    \"subsection\": \"subsection\",\n",
    "    \"paragraph\": \"paragraph\",\n",
    "    \"appendix\": \"appendix\",\n",
    "    \"item\": \"item\",\n",
    "}\n",
    "\n",
    "\n",
    "class NodeReference(BaseModel):\n",
    "    type: Literal[\"law\", \"part\", \"chapter\", \"section\", \"subsection\", \"paragraph\", \"appendix\", \"item\"]\n",
    "    title: str\n",
    "\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    title: str\n",
    "    effective_date: Annotated[str, Field(description=\"Effective date of the law\")]\n",
    "    references: Annotated[\n",
    "        list[NodeReference], Field(description=\"References to other laws or sections\")\n",
    "    ]\n",
    "    agencies: Annotated[\n",
    "        list[str], Field(description=\"Agencies or organizations mentioned\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_metadata(content, node_type, title):\n",
    "    \"\"\"\n",
    "    Extract metadata from content for Neo4j graph loading.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a metadata extraction assistant. Your task is to extract metadata from a legal document's content. \"\n",
    "        \"Please extract the following fields if they are present in the text. If a field is missing, return 'None' for that field.\\n\\n\"\n",
    "        \"Required metadata fields:\\n\"\n",
    "        \"- title: The full title of the legal document.\\n\"\n",
    "        \"- effective_date: The date on which the document becomes legally effective.\\n\"\n",
    "        \"- references: Any other laws, regulations, or documents explicitly mentioned in the text.\\n\"\n",
    "        \"- agencies: The names of government agencies, departments, or regulatory bodies that issued or are involved in the document.\\n\\n\"\n",
    "        \"The content is: {content}.\"\n",
    "    )\n",
    "    response = llm.with_structured_output(Metadata).invoke(prompt.format(content=content))\n",
    "    return response.model_dump_json()\n",
    "\n",
    "\n",
    "# Determine the level of a heading (1: part/appendix, 2: chapter, 3: section, 4: subsection, 5: paragraph)\n",
    "def get_level(line):\n",
    "    if re.match(part_pattern, line):\n",
    "        return 1  # Part\n",
    "    elif re.match(appendix_pattern, line):\n",
    "        return 1  # Appendix\n",
    "    elif re.match(chapter_pattern, line):\n",
    "        return 2  # Chapter\n",
    "    elif re.match(section_pattern, line):\n",
    "        return 3  # Section\n",
    "    elif re.match(subsection_pattern, line):\n",
    "        return 4  # Subsection (ODDÍL)\n",
    "    elif re.match(paragraph_pattern, line):\n",
    "        return 5  # Paragraph\n",
    "    return -1  # Not a heading\n",
    "\n",
    "\n",
    "def convert_law_to_json(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.rstrip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "    # Find the index of the first \"ČÁST\" or \"Příloha\"\n",
    "    first_part_index = next(\n",
    "        (\n",
    "            i\n",
    "            for i, line in enumerate(lines)\n",
    "            if re.match(part_pattern, line) or re.match(appendix_pattern, line)\n",
    "        ),\n",
    "        len(lines),\n",
    "    )\n",
    "    law_content = \"\\n\".join(lines[:first_part_index])\n",
    "\n",
    "    # Create the top-level \"law\" node\n",
    "    law_node = {\n",
    "        \"type\": \"law\",\n",
    "        \"title\": \"ZÁKON\",\n",
    "        \"content\": law_content,\n",
    "        \"metadata\": extract_metadata(law_content, \"law\", \"ZÁKON\"),\n",
    "        \"children\": [],\n",
    "    }\n",
    "\n",
    "    # Track current nodes at each level: 0=law, 1=part/appendix, 2=chapter, 3=section, 4=subsection, 5=paragraph\n",
    "    current_nodes = [law_node, None, None, None, None, None]\n",
    "    last_node = law_node\n",
    "    current_content = \"\"\n",
    "    in_subsection = False\n",
    "    current_subsection = None\n",
    "    in_appendix = False\n",
    "\n",
    "    # Process lines starting from the first \"ČÁST\" or \"Příloha\"\n",
    "    for line in tqdm.tqdm(lines[first_part_index:], desc=\"Processing lines\"):\n",
    "        level = get_level(line)\n",
    "        if level >= 1:  # Heading detected\n",
    "            # Finalize any ongoing subsection\n",
    "            if in_subsection and current_subsection:\n",
    "                current_subsection[\"content\"] = current_content.strip()\n",
    "                current_subsection[\"metadata\"] = extract_metadata(\n",
    "                    current_subsection[\"content\"],\n",
    "                    \"subsection\",\n",
    "                    current_subsection[\"title\"],\n",
    "                )\n",
    "                current_content = \"\"\n",
    "                in_subsection = False\n",
    "                current_subsection = None\n",
    "\n",
    "            # Handle appendix items\n",
    "            if in_appendix and current_content and last_node[\"type\"] == \"appendix\":\n",
    "                # Split current_content into items (assuming one per line)\n",
    "                items = current_content.strip().split(\"\\n\")\n",
    "                for item in items:\n",
    "                    if item and not re.match(\n",
    "                        subsubsection_pattern, item\n",
    "                    ):  # Exclude subsubsection-like lines\n",
    "                        item_node = {\n",
    "                            \"type\": \"item\",\n",
    "                            \"title\": item,\n",
    "                            \"content\": \"\",\n",
    "                            \"metadata\": extract_metadata(\"\", \"item\", item),\n",
    "                            \"children\": [],\n",
    "                        }\n",
    "                        last_node[\"children\"].append(item_node)\n",
    "                current_content = \"\"\n",
    "\n",
    "            # Attach accumulated content to the last node\n",
    "            if current_content and last_node and last_node[\"type\"] != \"appendix\":\n",
    "                last_node[\"content\"] = current_content.strip()\n",
    "                last_node[\"metadata\"] = extract_metadata(\n",
    "                    current_content, last_node[\"type\"], last_node[\"title\"]\n",
    "                )\n",
    "                current_content = \"\"\n",
    "\n",
    "            # Create new node\n",
    "            node_type = (\n",
    "                \"appendix\"\n",
    "                if re.match(appendix_pattern, line)\n",
    "                else [\"part\", \"chapter\", \"section\", \"subsection\", \"paragraph\"][\n",
    "                    level - 1\n",
    "                ]\n",
    "            )\n",
    "            new_node = {\n",
    "                \"type\": node_type,\n",
    "                \"title\": line,\n",
    "                \"content\": \"\",\n",
    "                \"metadata\": {},\n",
    "                \"children\": [],\n",
    "            }\n",
    "\n",
    "            # Find parent: highest level < current level with a node\n",
    "            parent_level = level - 1\n",
    "            while parent_level >= 0 and current_nodes[parent_level] is None:\n",
    "                parent_level -= 1\n",
    "            parent = current_nodes[parent_level] if parent_level >= 0 else law_node\n",
    "\n",
    "            # Add new node to parent's children\n",
    "            parent[\"children\"].append(new_node)\n",
    "            current_nodes[level] = new_node\n",
    "            for m in range(level + 1, 6):  # Reset lower levels\n",
    "                current_nodes[m] = None\n",
    "\n",
    "            last_node = new_node\n",
    "            in_appendix = node_type == \"appendix\"\n",
    "        else:\n",
    "            # Check for subsection (subsubsection) start\n",
    "            match = re.match(subsubsection_pattern, line)\n",
    "            if match and current_nodes[5]:  # Inside a paragraph\n",
    "                if in_subsection and current_subsection:\n",
    "                    # Finalize previous subsection\n",
    "                    current_subsection[\"content\"] = current_content.strip()\n",
    "                    current_subsection[\"metadata\"] = extract_metadata(\n",
    "                        current_subsection[\"content\"],\n",
    "                        \"subsection\",\n",
    "                        current_subsection[\"title\"],\n",
    "                    )\n",
    "                    current_content = \"\"\n",
    "\n",
    "                # Start new subsubsection\n",
    "                number, first_line_content = match.group(1), match.group(2) or \"\"\n",
    "                current_subsection = {\n",
    "                    \"type\": \"subsection\",\n",
    "                    \"title\": f\"({number})\",\n",
    "                    \"content\": first_line_content,\n",
    "                    \"metadata\": extract_metadata(\n",
    "                        first_line_content, \"subsection\", f\"({number})\"\n",
    "                    ),\n",
    "                    \"children\": [],\n",
    "                }\n",
    "                current_nodes[5][\"children\"].append(current_subsection)\n",
    "                in_subsection = True\n",
    "                current_content = (\n",
    "                    \"\" if not first_line_content else first_line_content + \"\\n\"\n",
    "                )\n",
    "            else:\n",
    "                # Accumulate content for current subsection, appendix, or node\n",
    "                if in_subsection:\n",
    "                    current_content += line + \"\\n\"\n",
    "                elif in_appendix:\n",
    "                    current_content += line + \"\\n\"\n",
    "                else:\n",
    "                    current_content += line + \"\\n\"\n",
    "\n",
    "    # Finalize any remaining content\n",
    "    if in_subsection and current_subsection:\n",
    "        current_subsection[\"content\"] = current_content.strip()\n",
    "        current_subsection[\"metadata\"] = extract_metadata(\n",
    "            current_subsection[\"content\"], \"subsection\", current_subsection[\"title\"]\n",
    "        )\n",
    "    elif in_appendix and current_content and last_node[\"type\"] == \"appendix\":\n",
    "        # Process remaining appendix items\n",
    "        items = current_content.strip().split(\"\\n\")\n",
    "        for item in items:\n",
    "            if item and not re.match(subsubsection_pattern, item):\n",
    "                item_node = {\n",
    "                    \"type\": \"item\",\n",
    "                    \"title\": item,\n",
    "                    \"content\": \"\",\n",
    "                    \"metadata\": extract_metadata(\"\", \"item\", item),\n",
    "                    \"children\": [],\n",
    "                }\n",
    "                last_node[\"children\"].append(item_node)\n",
    "    elif current_content and last_node:\n",
    "        last_node[\"content\"] = current_content.strip()\n",
    "        last_node[\"metadata\"] = extract_metadata(\n",
    "            current_content, last_node[\"type\"], last_node[\"title\"]\n",
    "        )\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(law_node, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"JSON saved to '{output_file}'\")\n",
    "\n",
    "\n",
    "INPUT_FOLDER = \"./output/text_data\"\n",
    "OUTPUT_FOLDER = \"./output/json_data\"\n",
    "\n",
    "pdf_files = [\n",
    "    os.path.join(INPUT_FOLDER, fname)\n",
    "    for fname in os.listdir(INPUT_FOLDER)\n",
    "    if fname.endswith(\".txt\")\n",
    "]\n",
    "\n",
    "# for pdf_file in pdf_files:\n",
    "#     output_path = os.path.join(\n",
    "#         OUTPUT_FOLDER, os.path.basename(pdf_file).replace(\".txt\", \".json\")\n",
    "#     )\n",
    "#     convert_law_to_json(pdf_file, output_path)\n",
    "\n",
    "convert_law_to_json(\n",
    "    \"./output/text_data/Sb_1991_455_2024-01-01_IZ.txt\",\n",
    "    \"./output/json_data/Sb_1991_455_2024-01-01_IZ.json\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
